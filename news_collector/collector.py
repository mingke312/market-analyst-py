#!/usr/bin/env python3
"""
æ–°é—»æ•°æ®æ”¶é›†æ¨¡å— v2
æ”¯æŒå¤šä¸ªæ–°é—»æºï¼šæ–°æµªè´¢ç»ã€å‡¤å‡°è´¢ç»
"""

import re
import json
from typing import Dict, List
from datetime import datetime
import urllib.request
import urllib.parse


class NewsCollector:
    """æ–°é—»æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.sources = []
    
    def fetch_from_sina(self) -> List[Dict]:
        """ä»æ–°æµªè´¢ç»è·å–æ–°é—»"""
        results = []
        
        try:
            encoded_keyword = urllib.parse.quote('Aè‚¡')
            url = f"https://search.sina.com.cn/?q={encoded_keyword}&c=news&sort=time"
            
            req = urllib.request.Request(url, headers=self.headers)
            
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                
                # æå–æ ‡é¢˜å’Œé“¾æ¥
                pattern = r'<a href="(https?://[^"]+)"[^>]*>([^<]+)</a>'
                matches = re.findall(pattern, content)
                
                seen = set()
                for url, title in matches[:15]:
                    if title.strip() and len(title) > 10 and url not in seen:
                        if 'sina.com.cn' in url or 'finance.sina' in url:
                            seen.add(url)
                            category = self._classify_news(title, '')
                            results.append({
                                'title': title.strip()[:100],
                                'url': url,
                                'source': 'æ–°æµªè´¢ç»',
                                'category': category,
                                'importance': self._get_importance(title, ''),
                                'timestamp': datetime.now().isoformat(),
                                'summary': '',
                            })
                            
        except Exception as e:
            print(f"æ–°æµªè´¢ç»è·å–å¤±è´¥: {e}")
        
        return results
    
    def fetch_from_phoenix(self) -> List[Dict]:
        """ä»å‡¤å‡°è´¢ç»è·å–æ–°é—»"""
        results = []
        
        try:
            # å‡¤å‡°ç½‘è´¢ç»é¢‘é“
            url = "https://news.ifeng.com/"
            
            req = urllib.request.Request(url, headers=self.headers)
            
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                
                # æå–è´¢ç»ç›¸å…³æ–°é—»
                pattern = r'<a href="(https?://[^\s]+)"[^>]*title="([^"]+)"[^>]*>'
                matches = re.findall(pattern, content)
                
                seen = set()
                for url, title in matches[:20]:
                    title = title.strip()
                    # è¿‡æ»¤ï¼šé•¿åº¦åˆé€‚ã€æ˜¯è´¢ç»ç›¸å…³å†…å®¹
                    if (title and len(title) >= 10 and len(title) <= 80 and 
                        url not in seen and 'ifeng.com' in url):
                        
                        # è¿‡æ»¤æ— å…³é“¾æ¥
                        if any(x in url for x in ['finance', 'stock', 'money', 'biz', 'news']):
                            seen.add(url)
                            category = self._classify_news(title, '')
                            results.append({
                                'title': title[:100],
                                'url': url,
                                'source': 'å‡¤å‡°è´¢ç»',
                                'category': category,
                                'importance': self._get_importance(title, ''),
                                'timestamp': datetime.now().isoformat(),
                                'summary': '',
                            })
                            
        except Exception as e:
            print(f"å‡¤å‡°è´¢ç»è·å–å¤±è´¥: {e}")
        
        return results
    
    def fetch_from_eastmoney(self) -> List[Dict]:
        """ä»ä¸œæ–¹è´¢å¯Œè·å–æ–°é—»"""
        results = []
        
        try:
            url = "https://stock.eastmoney.com/"
            
            req = urllib.request.Request(url, headers=self.headers)
            
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                
                # æå–æ–°é—»æ ‡é¢˜
                pattern = r'title="([^"]+)"'
                matches = re.findall(pattern, content)
                
                seen = set()
                for title in matches:
                    title = title.strip()
                    # è¿‡æ»¤ï¼šé•¿åº¦åˆé€‚ã€æ˜¯è´¢ç»ç›¸å…³å†…å®¹
                    if (title and 10 <= len(title) <= 60 and title not in seen):
                        
                        # è¿‡æ»¤æ— å…³æ ‡é¢˜
                        if any(x in title for x in ['è‚¡', 'æ¿å—', 'æ¶¨åœ', 'è·Œåœ', 'æŒ‡æ•°', 'æœŸè´§', 'å®è§‚', 'æ”¿ç­–', 'è´¢æŠ¥', 'ä¸šç»©', 'Aè‚¡', 'ç¾è‚¡', 'æ¸¯è‚¡']):
                            seen.add(title)
                            category = self._classify_news(title, '')
                            results.append({
                                'title': title[:100],
                                'url': "https://stock.eastmoney.com/",
                                'source': 'ä¸œæ–¹è´¢å¯Œ',
                                'category': category,
                                'importance': self._get_importance(title, ''),
                                'timestamp': datetime.now().isoformat(),
                                'summary': '',
                            })
                            
        except Exception as e:
            print(f"ä¸œæ–¹è´¢å¯Œè·å–å¤±è´¥: {e}")
        
        return results
    
    def fetch_from_wallstreetcn(self) -> List[Dict]:
        """ä»åå°”è¡—è§é—»è·å–æ–°é—»ï¼ˆéœ€è¦è®¤è¯ï¼Œä»…å°è¯•ï¼‰"""
        results = []
        
        try:
            # å°è¯•RSSæˆ–å…¬å¼€API
            url = "https://www.wallstreetcn.com/news"
            
            req = urllib.request.Request(url, headers=self.headers)
            
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read().decode('utf-8', errors='ignore')
                
                # å°è¯•æå–æ–°é—»æ ‡é¢˜
                patterns = [
                    r'<a[^>]*href="/news/([^"]+)"[^>]*>([^<]+)</a>',
                    r'"title":"([^"]+)"',
                ]
                
                for pattern in patterns:
                    matches = re.findall(pattern, content)
                    for match in matches[:10]:
                        if isinstance(match, tuple):
                            title = match[1] if len(match) > 1 else match[0]
                        else:
                            title = match
                        
                        title = title.strip()
                        if title and 10 <= len(title) <= 80:
                            results.append({
                                'title': title[:100],
                                'url': f"https://www.wallstreetcn.com/news/{match[0] if isinstance(match, tuple) else ''}",
                                'source': 'åå°”è¡—è§é—»',
                                'category': self._classify_news(title, ''),
                                'importance': self._get_importance(title, ''),
                                'timestamp': datetime.now().isoformat(),
                                'summary': '',
                            })
                        break
                            
        except Exception as e:
            print(f"åå°”è¡—è§é—»è·å–å¤±è´¥: {e}")
        
        return results
    
    def fetch_all(self) -> List[Dict]:
        """ä»æ‰€æœ‰æºè·å–æ–°é—»"""
        all_news = []
        
        # æ–°æµªè´¢ç»
        sina_news = self.fetch_from_sina()
        all_news.extend(sina_news)
        print(f"æ–°æµªè´¢ç»: {len(sina_news)}æ¡")
        
        # å‡¤å‡°è´¢ç»
        phoenix_news = self.fetch_from_phoenix()
        all_news.extend(phoenix_news)
        print(f"å‡¤å‡°è´¢ç»: {len(phoenix_news)}æ¡")
        
        # ä¸œæ–¹è´¢å¯Œ
        eastmoney_news = self.fetch_from_eastmoney()
        all_news.extend(eastmoney_news)
        print(f"ä¸œæ–¹è´¢å¯Œ: {len(eastmoney_news)}æ¡")
        
        # åå°”è¡—è§é—» (å¯èƒ½å¤±è´¥)
        try:
            wsnews = self.fetch_from_wallstreetcn()
            all_news.extend(wsnews)
            print(f"åå°”è¡—è§é—»: {len(wsnews)}æ¡")
        except:
            pass
        
        # å»é‡ (ä½¿ç”¨URLå’Œæ ‡é¢˜ç»„åˆ)
        seen = set()
        unique_results = []
        for news in all_news:
            # ä½¿ç”¨URL+æ ‡é¢˜ä½œä¸ºå”¯ä¸€æ ‡è¯†
            key = (news['url'], news['title'][:30])
            if key not in seen:
                seen.add(key)
                unique_results.append(news)
        
        return unique_results[:20]
    
    def _classify_news(self, title: str, content: str) -> str:
        """åˆ†ç±»æ–°é—»"""
        text = (title + content).lower()
        
        if any(kw in text for kw in ['é™æ¯', 'é™å‡†', 'åŠ æ¯', 'é€šèƒ€', 'gdp', 'ç»æµ', 'æ”¿ç­–', 'å¤®è¡Œ', 'è´¢æ”¿éƒ¨', 'è¯ç›‘ä¼š', 'è´§å¸']):
            return 'å®è§‚æ”¿ç­–'
        elif any(kw in text for kw in ['ç¾è‚¡', 'æ¸¯è‚¡', 'ç¾è”å‚¨', 'æ¬§æ´²', 'æ—¥æœ¬', 'éŸ©å›½', 'å…³ç¨', 'è´¸æ˜“', 'ç‰¹æœ—æ™®', 'æ‹œç™»']):
            return 'å›½é™…å¸‚åœº'
        elif any(kw in text for kw in ['æ¶¨åœ', 'è·Œåœ', 'å¹¶è´­', 'é‡ç»„', 'ä¸Šå¸‚', 'ipo', 'è´¢æŠ¥', 'ä¸šç»©', 'åˆ†çº¢', 'aè‚¡', 'è‚¡å¸‚', 'å¤§ç›˜', 'æŒ‡æ•°']):
            return 'å…¬å¸é‡å¤§äº‹é¡¹'
        elif any(kw in text for kw in ['æ–°èƒ½æº', 'åŠå¯¼ä½“', 'åŒ»è¯', 'é“¶è¡Œ', 'åœ°äº§', 'æ±½è½¦', 'ç§‘æŠ€', 'ai', 'äººå·¥æ™ºèƒ½', 'èŠ¯ç‰‡', 'å…‰ä¼']):
            return 'è¡Œä¸šåŠ¨æ€'
        else:
            return 'å…¶ä»–'
    
    def _get_importance(self, title: str, content: str) -> str:
        """åˆ¤æ–­é‡è¦æ€§"""
        text = (title + content).lower()
        
        high_importance = ['å¤®è¡Œ', 'é™æ¯', 'é™å‡†', 'åŠ æ¯', 'å…³ç¨', 'é‡å¤§', 'æ¶¨åœ', 'è·Œåœ', 
                          'çªå‘', 'é‡ç£…', 'åˆ©å¥½', 'åˆ©ç©º', 'æ”¿ç­–', 'ç›‘ç®¡', 'è¯ç›‘ä¼š', 'ç¾è‚¡',
                          'å´©ç›˜', 'æš´æ¶¨', 'å¤§è·Œ', 'çªç ´', 'å†å²']
        
        if any(kw in text for kw in high_importance):
            return 'é«˜'
        return 'ä¸­'
    
    def collect_all(self) -> Dict:
        """æ”¶é›†æ‰€æœ‰æ–°é—»"""
        data = self.fetch_all()
        
        return {
            'date': datetime.now().strftime("%Y-%m-%d"),
            'type': 'news',
            'sources': ['æ–°æµªè´¢ç»', 'å‡¤å‡°è´¢ç»', 'åå°”è¡—è§é—»'],
            'count': len(data),
            'data': data,
            'timestamp': datetime.now().isoformat()
        }


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import sys
    
    collector = NewsCollector()
    result = collector.collect_all()
    
    if len(sys.argv) > 1 and sys.argv[1] == '--json':
        print(json.dumps(result, ensure_ascii=False, indent=2))
    else:
        print(f"ğŸ“° æ–°é—»æ•°æ® ({result['count']}æ¡)")
        print("-" * 50)
        
        # æŒ‰æ¥æºç»Ÿè®¡
        sources_count = {}
        for item in result['data']:
            src = item['source']
            sources_count[src] = sources_count.get(src, 0) + 1
        
        print(f"æ¥æº: {sources_count}")
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for item in result['data']:
            cat = item['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"åˆ†ç±»: {categories}")
        
        print("-" * 50)
        print("\né‡è¦æ–°é—»:")
        for item in result['data'][:5]:
            if item['importance'] == 'é«˜':
                print(f"â­ [{item['source']}] {item['title'][:50]}")
                print(f"   [{item['category']}]")


if __name__ == "__main__":
    main()
